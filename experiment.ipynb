{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from envs import ObservableEnv, env_gridworld\n",
    "import rl.mdp as mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slides Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Environment as Numpy Array\n",
    "\n",
    "* Reward:\n",
    "    A 2D array of the rewards of $[s, a]$\n",
    "\n",
    "* Transition:\n",
    "    A 3D array of the probability of reaching state $s'$ under state $s$ and taking aciton $a$\n",
    "    \n",
    "* Value:\n",
    "    A 1D array of the expected value at state $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['PU', 'PF', 'RU', 'RF']\n",
    "actions = ['S', 'A']\n",
    "rewards = np.array([[0, 0],\n",
    "                    [0, 0],\n",
    "                    [10, 10],\n",
    "                    [10, 10]])\n",
    "transitions = np.array([[[1, 0, 0, 0], [0.5, 0.5, 0, 0]],\n",
    "                        [[0.5, 0., 0., 0.5], [0., 1., 0., 0.]],\n",
    "                        [[0.5, 0., 0.5, 0.], [0.5, 0.5, 0., 0.]],\n",
    "                        [[0., 0., 0.5, 0.5], [0., 1., 0., 0.]]])\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ObservableEnv(states, actions, transitions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 90 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([31.58236192, 38.60127399, 44.02143387, 54.19885637]),\n",
       " array([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.array([0, 0, 10, 10])\n",
    "\n",
    "value_iteration = mdp.ValueIteration(env, value, gamma)\n",
    "value_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 2 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 2.025,  4.5  , 16.525, 21.025]), array([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.array([0, 0, 10, 10])\n",
    "policy = np.array([random.randint(0, 1) for _ in range(4)])\n",
    "\n",
    "policy_iteration = mdp.PolicyIteration(env, value, policy, gamma)\n",
    "policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 13 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([31.58334865, 38.60228564, 44.02239741, 54.19992339]),\n",
       " array([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.array([0, 0, 10, 10])\n",
    "policy = np.array([random.randint(0, 1) for _ in range(4)])\n",
    "n_evals = 5\n",
    "\n",
    "modified_policy_iteration = mdp.ModifiedPolicyIteration(env, value, policy, gamma, n_evals)\n",
    "modified_policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 1 Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Environment\n",
    "\n",
    "Grid world layout:\n",
    "\n",
    "  \\---------------------  \n",
    "  |  0 |  1 |  2 |  3 |  \n",
    "  \\---------------------  \n",
    "  |  4 |  5 |  6 |  7 |  \n",
    "  \\---------------------  \n",
    "  |  8 |  9 | 10 | 11 |  \n",
    "  \\---------------------  \n",
    "  | 12 | 13 | 14 | 15 |  \n",
    "  \\---------------------  \n",
    "\n",
    "  Goal state: 15   \n",
    "  Bad state: 9  \n",
    "  End state: 16  \n",
    " \n",
    "$|S| = 17$  \n",
    "$|A| = 4$\n",
    "\n",
    "transitions: $|S| * |A| * |S'|$  \n",
    "rewards = $|S| * |A|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [str(i) for i in range(17)]\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "transitions = env_gridworld.transtions\n",
    "rewards = env_gridworld.rewards\n",
    "\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ObservableEnv(states, actions, transitions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 23 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 39.66673616,  46.41953402,  54.06871129,  61.7166396 ,\n",
       "         40.83139756,  47.91550368,  62.96734881,  72.63330321,\n",
       "         41.80375928,  -6.65234296,  73.77496298,  85.31840171,\n",
       "         55.05578488,  65.74783919,  85.31840171, 100.        ,\n",
       "          0.        ]),\n",
       " array([3, 3, 1, 1, 3, 3, 1, 1, 1, 3, 3, 1, 3, 3, 3, 0, 0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.zeros(len(states))\n",
    "\n",
    "value_iteration = mdp.ValueIteration(env, value, gamma)\n",
    "value_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 9 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 49.41906608,  59.73407408,  68.96677157,  75.51410453,\n",
       "         51.79894022,  62.37622474,  76.42752305,  83.72598624,\n",
       "         55.21588678,   6.58968597,  84.5293075 ,  91.68488264,\n",
       "         67.92184379,  76.40994525,  91.68488264, 100.        ,\n",
       "          0.        ]),\n",
       " array([3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.zeros(len(states))\n",
    "policy = np.array([random.randint(0, len(actions) - 1) for _ in range(len(states))])\n",
    "\n",
    "policy_iteration = mdp.PolicyIteration(env, value, policy, gamma)\n",
    "policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 4 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 60.63220183,  66.03886109,  71.80619988,  77.09294032,\n",
       "         59.8192882 ,  65.18452884,  77.83150596,  84.14148782,\n",
       "         58.09554418,   7.9886184 ,  84.86730342,  91.78165054,\n",
       "         69.49679766,  76.80991419,  91.78165054, 100.        ,\n",
       "          0.        ]),\n",
       " array([3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 0, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.zeros(len(states))\n",
    "policy = np.array([random.randint(0, len(actions) - 1) for _ in range(len(states))])\n",
    "n_evals = 5\n",
    "\n",
    "modified_policy_iteration = mdp.ModifiedPolicyIteration(env, value, policy, gamma, n_evals)\n",
    "modified_policy_iteration.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
